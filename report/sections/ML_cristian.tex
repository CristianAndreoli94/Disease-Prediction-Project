% \subsection{Choosing Models for Disease Prediction}
% In the realm of disease prediction, selecting the right machine learning models is paramount to achieving accurate and meaningful results. Our dataset, consisting of 250,000 samples and encompassing 700 disease classes, presents unique challenges and opportunities. In this section, we delve into the rationale behind the selection of three distinct models: Random Forest, Multilayer Perceptron (MLP), and Logistic Regression.

% \subsubsection{Data Preprocessing}
% Before delving into model development, a robust data preprocessing pipeline was employed to enhance the effectiveness of our machine learning models.
% \begin{itemize}
%     \item \textbf{Random Sampling for Efficient Hyperparameter Tuning}: Given the extensive nature of hyperparameter tuning, we adopted a random sampling strategy during the optimization process. Instead of training on the entire dataset for each hyperparameter combination, a random subset was used to expedite the tuning phase without sacrificing model representativity.
%     \item \textbf{Addressing Class Imbalance with Oversampling and Undersampling}: The dataset, with its 700 disease classes, exhibited class imbalance. To mitigate this, a combination of oversampling and undersampling techniques was applied. Oversampling was performed for minority classes, ensuring that these classes were adequately represented during training. Concurrently, undersampling was applied to the majority classes, preventing dominance and bias in the model.
%     \item \textbf{Max-Abs Normalization for Sparse Feature Representation}: Considering the sparsity of our one-hot encoded symptom features (300 features representing 700 classes), a careful choice of normalization technique was crucial. We opted for max-abs normalization, preserving the sparsity of the data. This normalization method scales each feature by its maximum absolute value, ensuring that the sparsity pattern remains intact while achieving feature-wise scale consistency.
% \end{itemize}

% \subsubsection{Logistic Regression}
% \begin{itemize}
%     \item \textbf{Strengths}: Logistic Regression's computational efficiency makes it an attractive choice for initial exploration and baseline performance assessment. Its simplicity facilitates interpretability, providing insights into the impact of individual symptoms on disease prediction.
%     \item \textbf{Considerations}: While efficient, Logistic Regression assumes a linear relationship between features and the log-odds of the target, potentially limiting its ability to capture complex non-linear patterns.
% \end{itemize}

% \subsubsection{Random Forest}
% \begin{itemize}
%     \item \textbf{Strengths}: Random Forest is renowned for its robustness in handling large and diverse datasets, making it well-suited for our expansive dataset with 700 disease classes. Moreover, Its ability to capture non-linear relationships ensures that complex patterns within the symptoms' one-hot encoded features are effectively modeled.
%     \item \textbf{Considerations}: The ensemble nature of Random Forest provides resilience against overfitting, a crucial factor in the context of disease prediction.
% \end{itemize}

% \subsubsection{Multilayer Perceptron (MLP)}
% \begin{itemize}
%     \item \textbf{Strengths}: MLPs are adept at capturing intricate relationships in high-dimensional datasets, aligning with the complexity inherent in our 300-feature symptom representation.
%     \item \textbf{Considerations}: Their capacity for adapting to non-linear mappings positions MLPs as powerful tools in unraveling the nuanced interactions between symptoms and diseases.
% \end{itemize}


% ---------- THIS HAS BEEN REWRITTEN IN ALGORITHMIC FASHION ---------


% \subsubsection{Parameter Tuning}
% To optimize the performance of our machine learning models, we initiated the process by conducting parameter tuning for models using symptom features represented as a one-hot vector. Given the high-dimensional nature of our dataset, a thoughtful approach was taken to efficiently explore the hyperparameter space.
% \begin{itemize}
%     \item \textbf{Greedy Hyperparameter Optimization}: Rather than exhaustively searching all combinations, we adopted a greedy approach for hyperparameter optimization. One parameter at a time was systematically varied while keeping others constant. This allowed us to efficiently identify key hyperparameters influencing model performance.
%     \item \textbf{Cross-Validation}: The models were subjected to cross-validation techniques, ensuring that the selected hyperparameters demonstrated robust performance across multiple folds. This approach provided a balance between optimization efficiency and model generalization.
% \end{itemize}
% Expanding our approach beyond symptom features, we incorporated a combination of other features in the models. The parameter tuning process for this scenario involved a randomized search across a broader hyperparameter space.
% \begin{itemize}
%     \item \textbf{Randomized Hyperparameter Search}: Random parameter combinations were sampled to efficiently explore the high-dimensional space of hyperparameters. This approach allowed us to identify promising configurations for models using combined features.
%     \item \textbf{Performance Evaluation}: The models were rigorously evaluated based on accuracy. We assessed their ability to generalize to unseen data and make accurate predictions across the diverse set of disease classes.
% \end{itemize}

% \subsubsection{Training and Model Selection}
% Following the parameter tuning phase, we proceeded with the training of multiple model instances.
% \begin{itemize}
%     \item \textbf{Model Training}: Each model configuration, both for symptom features only and combined features, was trained on the entire dataset. Training involved the application of optimal hyperparameters determined through the tuning process.
%     \item \textbf{Performance Comparison}: The models were comprehensively compared based on their accuracy and other relevant metrics. This step allowed us to identify the model configuration that demonstrated the highest predictive performance.
% \end{itemize}


\subsubsection{Hyperparameter Tuning Strategy: A Resource-Efficient Greedy Search}

The quest for optimal hyperparameters in machine learning models is often constrained by computational resources. In light of these 
limitations, we adopted a resource-efficient greedy search strategy to navigate the vast hyperparameter space.
Our approach unfolds in several stages. Initially, we randomly initialize hyperparameters to initiate the search. Subsequently, we employ 
a stepwise exploration, beginning with the first hyperparameter. For this, we perform an initial search over a small set of values 
(e.g., 0.001, 0.01, 1, 10, 100). If the optimum lies at one of the extremes, we extend the search to encompass values in the corresponding 
direction.; conversely, if it resides within an intermediate range, we conduct a more focused exploration in a narrower interval. 
This process is iteratively repeated for each hyperparameter, gradually refining our understanding of the optimal regions within the 
hyperparameter space.
This iterative approach serves a dual purpose. First, it conserves computational resources by avoiding an exhaustive search over all 
potential combinations. Second, it capitalizes on the information gleaned from earlier iterations to guide subsequent searches efficiently. 
By strategically determining the next set of values based on the observed trends, we strike a balance between exploration and exploitation, 
ultimately converging to a set of hyperparameters that maximizes model performance. This resource-conscious strategy is paramount when 
computational resources are limited, allowing us to derive meaningful results within practical constraints.

\begin{algorithm}
    \caption{Greedy Hyperparameter Search}\label{alg:greedy_hyperparameter_search}
    \begin{algorithmic}[1]
        \State hyperparameters $\gets$ randomInitialization
        
        \For{each hyperparameter in hyperparameters}
            \State search\_range $\gets$ initialSearchRange
            \State best\_value $\gets$ current\_hyperparameters[hyperparameter]
            \State accuracy $\gets$ 0
            
            \For{value in search\_range}
                \State current\_hyperparameters[hyperparameter] $\gets$ value
                \State performance $\gets$ evaluateModel(model, current\_hyperparameters)
                
                \If{performance better than accuracy}
                    \State best\_value $\gets$ value
                    \State accuracy $\gets$ performance
                \EndIf
            \EndFor
            
            \If{best\_value is at the lower extreme}
                \State search\_range $\gets$ extendSearchRangeLower(best\_value)
            \ElsIf{best\_value is at the upper extreme}
                \State search\_range $\gets$ extendSearchRangeUpper(best\_value)
            \Else
                \State search\_range $\gets$ narrowSearchRange(best\_value)
            \EndIf
            
            \For{value in search\_range}
                \State current\_hyperparameters[hyperparameter] $\gets$ value
                \State performance $\gets$ evaluateModel(model, current\_hyperparameters)
                
                \If{performance better than accuracy}
                    \State best\_value $\gets$ value
                    \State accuracy $\gets$ performance
                \EndIf
            \EndFor
            \State current\_hyperparameters[hyperparameter] $\gets$ best\_value
        \EndFor
    \end{algorithmic}
\end{algorithm}



https://www.sciencedirect.com/science/article/pii/S2352914823001624
https://www.sciencedirect.com/science/article/pii/S2666990021000318



\rowcolors{2}{blue!8}{blue!18}
\begin{table}[H]
	\centering
	\small
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{Hyperparameters}                    & \textbf{Test} & \textbf{Best} \\
		\hline
		C                                   & 0.001, 0.01, 0.1, 0.5, 0.75, 1, 1.25, 1.50, 10, 100                & 1.5          \\
		max_iter                            & 100, 200, 300, 500, 1000               & Until Convergence          \\
		penalty                             & l1, l2, None               & l2          \\
		solver                              & lbfgs, liblinear, newton-cg               & lbfgs         \\
		\hline
	\end{tabular}
	\caption{Best Hyperparameters selection}
	\label{best}
\end{table}

\rowcolors{2}{blue!8}{blue!18}
\begin{table}[H]
	\centering
	\small
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{Hyperparameters}                    & \textbf{Test} & \textbf{Best} \\
		\hline
		n_estimators                                   & 50, 80, 100, 200, 300, 400, 500, 600              & 600          \\
		max_depth                        & 25, 50, 60, 75, 100, None             & 50        \\
		min_samples_split                           & 2, 5, 10, 20              & 2          \\
		min_samples_leaf                              & 1, 2, 5, 10            & 1         \\
		\hline
	\end{tabular}
	\caption{Best Hyperparameters selection}
	\label{best}
\end{table}

\rowcolors{2}{blue!8}{blue!18}
\begin{table}[H]
	\centering
	\small
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{Hyperparameters}                    & \textbf{Test} & \textbf{Best} \\
		\hline
		hidden_layer_sizes                               & (60), (80), (100), (200), (300), (400)                & (80)          \\
		max_iter                            & 100, 200, 300, 500, 1000               & Until Convergence \\
		alpha                             & 0.0001, 0.001, 0.01, 0.1, 1,               & 0.0001         \\
		activation                              & relu, tanh, logistic, identity              & relu         \\
		\hline
	\end{tabular}
	\caption{Best Hyperparameters selection}
	\label{best}
\end{table}